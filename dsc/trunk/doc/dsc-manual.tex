\documentclass{report}
\usepackage{epsfig}
\usepackage{path}
\usepackage{fancyvrb}

\def\dsc{{\sc dsc}}

\begin{document}

\begin{titlepage}
\title{DSC Manual}
\author{Duane Wessels\\
The Measurement Factory, Inc.\\
\\
http://dns.measurement-factory.com/tools/dsc/}
\date{\today}
\end{titlepage}

\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

{\dsc} is a system for collecting and presenting statistics from
a busy DNS server.  

\section{Components}

{\dsc} consists of the following components:
\begin{itemize}
\item A data collector
\item A data presenter, where data is archived and rendered
\item A method for securely transferring data from the collector
	to the presenter
\item Utilities and scripts that parse XML and archive files from the collector
\item Utilities and scripts that generate graphs and HTML pages
\end{itemize}

\subsection{The Collector}

The collector is a binary program, named {\tt dsc\/}, which snoops
on DNS messages.  It is written in C and uses {\em libpcap\/} for
packet capture.

{\tt dsc\/} uses a relatively simple configuration file called {\em
dsc.conf\/} to define certain parameters and options.  The configuration
file also determines the {\em datasets\/} that {\tt dsc\/} collects.

A Dataset is a 2-D array of counters of IP/DNS message properties.
You can define each dimension of the arrray independently.  For
example you might define a dataset categorized by DNS query type
along one dimension and TLD along the other.

{\tt dsc\/} dumps the datasets from memory to XML files every 60 seconds.

\subsection{XML Data Transfer}

You may run the {\dsc} collector on a remote machine.  That
is, the collector may run on a different machine than where the
data is archived and displayed.  {\dsc} includes some Perl and {\tt /bin/sh}
scripts that enable transporting XML files over a secure HTTP
connection.

To make this work, Apache+mod\_ssl should run on the machine where data
is archived and presented.
Data transfer is authenticated via SSL X.509 certificates.  A Perl
CGI script handles all PUT requests on the server.  If the client
certificate is allowed, XML files are stored in the appropriate
directory.

A shell script runs on the collector to upload the XML files.  It
uses {\tt curl\/}\footnote{http://curl.haxx.se} to establish an
HTTPS connection.  XML files are bundled together with {\tt tar\/}
before transfer to eliminate per-connection delays.

You could use {\tt scp\/} or {\tt rsync\/}/{\tt ssh\/} instead of
{\tt curl\/} if you like.

\path|put-file.pl| is the script that accepts PUT requests on the
HTTP server.  The HTTP server validates the client's X.509 certificate.
If the certificate is invalid, the PUT request is denied.  This
script reads environment variables to get X.509 parameters.  The
uploaded-data is stored in a directory based on the X.509 Organizational
Unit (server) and Common Name fields (node).

\subsection{The Extractor}

The XML extractor is a Perl script that reads the XML files from
{\tt dsc\/}.  The extractor essentially converts the XML-structured
data to a format that is easier (faster) for the graphing tools to
parse.  Currently the extracted data files are line-based ASCII
text files.  Support for SQL databases is planned for the future.

\subsection{The Grapher}

{\dsc} uses {\em Ploticus\/}\footnote{http://ploticus.sourceforge.net/}
as the graphing engine.  A Perl module and CGI script read extracted
data files and generate Ploticus scriptfiles to generate plots.  Plots
are always generated on demand via the CGI application.

\path|dsc-grapher.pl| is the script that displays graphs from the
archived data.  


\section{Architecture}

Figure~\ref{fig-architecture} shows the {\dsc} architecture.  

\begin{figure}
\centerline{\psfig{figure=dsc-arch.eps,width=3.5in}}
\caption{\label{fig-architecture}The {\dsc} architecture.}
\end{figure}

Note that {\dsc} utilizes the concept of {\em servers\/} and {\em
nodes\/}.  A server is generally a logical service, which may
actually consist of multiple nodes.  Figure~\ref{fig-architecture}
shows six collectors (the circles) and two servers (the rounded
rectangles).  For a real-world example, consider a DNS root server.
IP Anycast allows a DNS root server to have geographically distributed
nodes that share a single IP address.  We call each instance a 
{\em node\/} and all nodes sharing the single IP address belong
to the same {\em server\/}.

The {\dsc} collector program runs on or near\footnote{by
``near'' we mean that packets may be sniffed remotely via switch
port mirroring, or a SPAN port.} the remote nodes.  Its XML output
is transferred to the presentation machine via HTTPS PUTs (or something simpler
if you prefer).

The presentation machine includes an HTTP(S) server.  The extractor looks
for XML files PUT there by the collectors.  A CGI script also runs on
the HTTP server to display graphs and other information.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Installing the Presenter}

You'll probably want to get the Presenter working before the Collector.
If you're using the secure XML data transfer, you'll need to
generate X.509 both client- and server-side X.509 certificates.

Installing the Presenter involves the following steps:
\begin{itemize}
\setlength{\itemsep}{0ex plus 0.5ex minus 0.0ex}
\item
	Install Perl dependencies
\item
	Install {\dsc} software
\item
	Create X.509 certificates
\item
	Set up a secure HTTP server (e.g., Apache and mod\_ssl)
\item
	Add some cron jobs
\end{itemize}


\section{Install Perl Dependencies}

{\dsc} uses Perl for the extractor and grapher components.  Chances are
that you'll need Perl-5.8, or maybe only Perl-5.6.  You'll also need
these readily available third-party Perl modules, which you
can find via CPAN:

\begin{itemize}
\setlength{\itemsep}{0ex plus 0.5ex minus 0.0ex}
	\item CGI-Untaint (CGI::Untaint)
	\item CGI.pm (CGI)
	\item Digest-MD5 (Digest::MD5)
	\item File-Flock (File::Flock)
	\item File-Spec (File::Spec)
	\item File-Temp (File::Temp)
	\item Hash-Merge (Hash::Merge)
	\item MIME-Base64 (MIME::Base64)
	\item Math-Calc-Units (Math::Calc::Units)
	\item Scalar-List-Utils (List::Util)
	\item Text-Template (Text::Template)
	\item URI (URI::Escape)
	\item XML-Parser (XML::Parser)
	\item XML-Simple (XML::Simple)

\end{itemize}

\noindent
Also note that XML::Parser requires the {\em expat\/} package.

\section{Install {\dsc} Software}

All of the extractor and grapher tools are Perl or {\tt /bin/sh}
scripts, so there is no need to compile anything.  Still,
you should run {\tt make} first:

\begin{verbatim}
> cd presenter
> make
\end{verbatim}

If you see errors missing Perl prerequisites, you may want
to correct those before continuing.

The next step is to install the files.  Recall that
\path|/usr/local/dsc| is the hard-coded installation prefix.
You must create it manually:

\begin{verbatim}
> mkdir /usr/local/dsc
> make install
\end{verbatim}

Note that {\dsc}'s Perl modules are installed in the 
``site\_perl'' directory.  You'll probably need {\em root\/}
privileges to install files there.

\section{CGI Symbolic Links}

{\dsc} has a couple of CGI scripts that are installed
into \path|/usr/local/dsc/libexec|.  You should add symbolic
links from your HTTP server's \path|cgi-bin| directory to
these scripts.

Both of these scripts have been designed to be mod\_perl-friendly.

\begin{verbatim}
> cd /usr/local/apache/cgi-bin
> ln -s /usr/local/dsc/libexec/put-file.pl
> ln -s /usr/local/dsc/libexec/dsc-grapher.pl
\end{verbatim}

If you cannot create symbolic links, you'll need to manually
copy the scripts to the appropriate directory.


\section{/usr/local/dsc/data}

This directory is where \path|put-file.pl| writes incoming XML
files.  Actually, XML files are placed in {\em server\/} and {\em
node\/} subdirectories based on the authorized client X.509 certificate
parameters.  If you want \path|put-file.pl| to automatically create
the subdirectories, the \path|data| directory must be writable by
the process owner:

\begin{verbatim}
> mkdir /usr/local/dsc/data/
> chgrp nobody /usr/local/dsc/data/
> chmod 2775 /usr/local/dsc/data/
\end{verbatim}

Alternatively, you can create {\em server\/} and {\em node\/} directories
in advance and make those writable.

\begin{verbatim}
> mkdir /usr/local/dsc/data/
> mkdir /usr/local/dsc/data/x-root/
> mkdir /usr/lodal/dsc/data/x-root/blah/
> chgrp nobody /usr/local/dsc/data/x-root/blah/
> chmod 2775 /usr/local/dsc/data/x-root/blah/
\end{verbatim}

Make sure that \path|/usr/local/dsc/data/| is on a large partition with
plenty of free space.  You can make it a symbolic link to another
parition if necessary.


\section{\tt cron}

The {\em cron\/} directory contains a handful of useful scripts that you can
call from {\tt cron\/}.  When you type {\tt make install\/} these are copied
to {\em /usr/local/dsc/libexec\/}.

The {\em refile-and-grok.sh\/} script should be called every 1--5 minutes to process
incoming XML files:

\begin{verbatim}
* * * * * /usr/local/dsc/libexec/refile-and-grok.sh
\end{verbatim}

That script looks for subdirectories in {\em /usr/local/dsc/data\/}
and then calls {\em refile-and-grok-node.sh\/} for each server,node tuple.
This separate script helps improve performance by processing all nodes
in parallel.  

The {\em refile-and-grok-node.sh\/} script looks for new XML files and passes
them to the appropriate XML extractor.  

The {\em mk-plots.sh\/} script may be called every 5--30 minutes to generate
the graphs.  It simply iterates through directories in
{\em /usr/local/dsc/htdocs\/} and calls the Makefiles for each server and node.

The {\em remove-xmls.pl\/} script may be executed daily to 
remove old XML files.  For example, if you want to keep only 10 days
worth of XML files, use this cron entry:

\begin{verbatim}
0 0 * * * /usr/local/dsc/libexec/remove-xmls.pl 10
\end{verbatim}



\section{/usr/local/dsc symlinks}

{\dsc} uses hard-coded directory names under {\em /usr/local/dsc\/}.
Incoming XML files are stored under {\em /usr/local/dsc/data\/}.
You can either create it as a directory, or make it a symbolic link
to another parition with more storage.

Scripts that create graphs and HTML pages use {\em /usr/local/dsc/htdocs\/}
as {\dsc}'s document root.  Again, this can either be a real directory
or a symbolic link to the correct location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Configuring the {\dsc} Presenter}

After installation of the {\dsc} files, there is not much additional
configuration to do.  You do need to configure the HTTP server, however.

\section{Apache Configuration}

\noindent
You need to configure Apache for SSL.  Here is what our configuration
looks like:

\begin{verbatim}
SSLRandomSeed startup builtin
SSLRandomSeed startup file:/dev/random
SSLRandomSeed startup file:/dev/urandom 1024
SSLRandomSeed connect builtin
SSLRandomSeed connect file:/dev/random
SSLRandomSeed connect file:/dev/urandom 1024

<VirtualHost _default_:443>
DocumentRoot "/httpd/htdocs-ssl"
SSLEngine on
SSLCertificateFile /httpd/conf/SSL/server/server.crt
SSLCertificateKeyFile /httpd/conf/SSL/server/server.key
SSLCertificateChainFile /httpd/conf/SSL/cacert.pem

# For client-validation
SSLCACertificateFile /httpd/conf/SSL/cacert.pem
SSLVerifyClient require

SSLOptions +CompatEnvVars
Script PUT /cgi-bin/put-file.pl
</VirtualHost>
\end{verbatim}

\noindent
Note the last line of the configuration specifies the CGI script
that accepts PUT requests.  The <emphasis>SSLOptions</emphasis>
line is necessary so that the CGI script receives certain HTTP
headers as environment variables.  Those headers/variables convey
the X.509 information to the script so it knows where to store
received XML files.

\section{Generating X.509 Certificates}

We use X.509 certificates to authenticate both sides
of an SSL connection when uploading XML data files from 
the collector to the archiver.

Certificate generation is a tricky thing.  We use three different
types of certificates:
\begin{enumerate}
\item A self-signed root CA certificate
\item A server certificate
\item Client certificates for each collector node
\end{enumerate}

In the client certificates
we use X.509 fields to store the collector's server and node name.
The Organizational Unit Name (OU) becomes the server name and
the Common Name (CN) becomes the node name:

\begin{verbatim}
% openssl req -newkey rsa:1024 -keyout new.key -out new.req
...
Country Name (2 letter code) [AU]:US
State or Province Name (full name) [Some-State]:Colorado
Locality Name (eg, city) []:Boulder
Organization Name (eg, company) [Internet Widgits Pty Ltd]:TMF
Organizational Unit Name (eg, section) []:x-root
Common Name (eg, YOUR name) []:BLDR
Email Address []:wessels@measurement-factory.com
\end{verbatim}

When the {\em put-file.pl\/} script receives an XML file over an SSL session
that uses this key, it will store the data in the {\em x-root/bldr\/} directory.

\subsection{Certificate Authority} 

You may need to create a self-signed certificate authority if you
don't already have one.  The CA signs client and server certificates.
You will need to distribute the CA and client certificates to
collector sites.    Figure~\ref{fig-create-ca-cert-sh} is a sample
script for creating the CA, and Figure~\ref{fig-openssl-conf} is
a sample \path|openssl.conf| file.

\begin{figure}
\begin{verbatim}
#!/bin/sh
set -e

X_OPENSSL_CONF=/usr/local/dsc/certs/openssl.conf

mkdir -p private

if test ! -f index.txt ; then touch index.txt ; fi
if test ! -f serial ; then echo 0 > serial ; fi

CA_KEY=private/cakey.pem
CA_CRT=cacert.pem

# create the CA's self-signed root certificate
#
if test ! -f $CA_CRT ; then
        echo 'CREATING CA CERT'
        env OPENSSL_CONF=$X_OPENSSL_CONF \
        openssl req -x509 -days 3000 -newkey rsa -out $CA_CRT
fi
\end{verbatim}
\caption{\label{fig-create-ca-cert-sh}A script to create a CA}
\end{figure}


\begin{figure}
\begin{verbatim}
[ ca ]
default_ca              = foo_ca

[ foo_ca ]
dir                     = /usr/local/dsc/certs
certificate             = $dir/cacert.pem
database                = $dir/index.txt
new_certs_dir           = $dir/certs
private_key             = $dir/private/cakey.pem
serial                  = $dir/serial
default_crl_days        = 7
default_days            = 3000
default_md              = md5
policy                  = foo_ca_policy
x509_extensions         = foo_ca_extensions

[ foo_ca_policy ]
commonName              = supplied
stateOrProvinceName     = supplied
countryName             = supplied
emailAddress            = supplied
organizationName        = supplied
organizationalUnitName  = optional

[ foo_ca_extensions ]
basicConstraints        = CA:false

[ req ]
default_bits            = 2048
default_keyfile         = /usr/local/dsc/certs/private/cakey.pem
default_md              = md5
prompt                  = no
distinguished_name      = foo_ca_distinguished_name
x509_extensions         = foo_ca_extensions

[ foo_ca_distinguished_name ]
commonName              = FOO-CA
stateOrProvinceName     = Kansas
countryName             = US
emailAddress            = root@example.com
organizationName        = FOO

[ foo_ca_extensions ]
basicConstraints        = CA:true
\end{verbatim}
\caption{\label{fig-openssl-conf}A sample openssl.conf file}
\end{figure}


\subsection{Client Certificates}

\begin{figure}
\begin{verbatim}
#!/bin/sh
set -e

X_OPENSSL_CONF=/usr/local/dsc/certs/openssl.conf

mkdir -p client

CLT_REQ=client/client.csr
CLT_KEY=client/client.key
CLT_CRT=client/client.crt

# create a certificate request for a client
#
if test ! -f $CLT_REQ ; then
        echo 'CREATING CLIENT REQUEST'
        openssl req -newkey rsa:1024 -keyout $CLT_KEY -out $CLT_REQ
fi

# issue a certificate based on the client's request
#
if test ! -f $CLT_CRT ; then
        echo 'CREATING CLIENT CERT'
        env OPENSSL_CONF=$X_OPENSSL_CONF \
        openssl ca -in $CLT_REQ -out $CLT_CRT
fi

openssl rsa -in $CLT_KEY -out $CLT_KEY.new
mv $CLT_KEY.new $CLT_KEY

CN=`openssl x509 -in $CLT_CRT -noout -subject -nameopt multiline | awk '/commonName/ {print $3}'`
OU=`openssl x509 -in $CLT_CRT -noout -subject -nameopt multiline | awk '/organizationalUnitName/ {p
rint $3}'`
mkdir -p client/$OU/$CN
mv $CLT_KEY client/$OU/$CN/client.key
mv $CLT_REQ client/$OU/$CN/client.csr
mv $CLT_CRT client/$OU/$CN/client.crt

openssl rsa -in client/$OU/$CN/client.key   > client/$OU/$CN/client.pem
openssl x509 -in client/$OU/$CN/client.crt >> client/$OU/$CN/client.pem

exit 0
\end{verbatim}
\caption{\label{fig-create-client-cert-sh}A script to create client certificates}
\end{figure}

We use a simlar script to generate client certificates.  It is shown
in Figure~\ref{fig-create-client-cert-sh}.

After running the {\tt create-client-cert.sh} script,
you should find a number of files under the
\path|client| directory.   For example:

\begin{verbatim}
> ls -R client/
x-root

client/f-root:
bldr

client/f-root/bldr:
client.crt      client.csr      client.key      client.pem
\end{verbatim}

The \path|client.pem| files should be copied to their respective
collector machines.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Collector Installation}


A collector machine needs only the {\em dsc\/} binary, a configuration file,
and a cron job script.

At this point, {\dsc} lacks certain niceties such as a \path|./configure|
script.   The installation prefix, \path|/usr/local/dsc| is currently
hard-coded.


\section{Prerequisites}

You'll need a C/C++ compiler to compile the {\tt dsc\/} source code.

If the collector and archiver are different systems, you'll need a
way to transfer data files.  We recommend that you use the {\tt
curl\/} HTTP/SSL client You may use another technique, such as {\tt
scp\/} or {\tt rsync\/} if you prefer.

\section{\tt Installation}

You can compile {\tt dsc\/} from the {\tt collector\/} directory:

\begin{verbatim}
% cd collector
% make
\end{verbatim}

Assuming there are no errors or problems during compliation, install
the {\tt dsc\/} binary and other scripts with:

\begin{verbatim}
% make install
\end{verbatim}

This installs three files:
\path|/usr/local/dsc/bin/dsc|,
\path|/usr/local/dsc/etc/dsc.conf.sample|, 
\path|/usr/local/dsc/libexec/upload-data.sh|, and
\path|/usr/local/dsc/libexec/upload-prep.sh|.

Of course, if you don't want to use the default installation
prefix, you can manually copy these files to a location
of your choosing.  @@cron scripts need to be edited then.

\section{Uploading XML Files} 
\label{sec-install-collector-cron}

This section describes how XML files are transferred from
the collector to one or more ``presenter'' systems.

As we'll see in the next chapter, each {\tt dsc} process
has its own {\em run directory\/}.  This is the directory
where {\tt dsc} leaves its XML files.  It usually has a
name like \path|/usr/local/dsc/run/NODENAME|.

In general we want to be able to upload XML files to multiple
presenters.  This is the reason behind the {\tt upload-prep.sh}
script.  This script runs every 60 seconds from cron:

\begin{verbatim}
* * * * * /usr/local/dsc/libexec/upload-prep.sh
\end{verbatim}

{\tt upload-prep.sh} looks for \path|dsc.conf| files in
\path|/usr/local/dsc/etc| by default.  For each config file
found, it cd's to the {\em run\_dir\/} and copies\footnote{Actually,
XML files are hard-linked, rather than copied, to save disk space.}
XML files to one or more upload directories.  The upload directories
are named \path|upload/dest1|, \path|upload/dest2|, and so on.

In order for all this to work, you must create the directories
in advance.   For example, if you are collecting stats on
your nameserver named {\em ns0\/} the directory structure might
look like:

\begin{verbatim}
> set prefix=/usr/local/dsc
> mkdir $prefix/run
> mkdir $prefix/run/ns0
> mkdir $prefix/run/ns0/upload
> mkdir $prefix/run/ns0/upload/oarc
> mkdir $prefix/run/ns0/upload/archive
\end{verbatim}

With that directory structure, the {\tt upload-prep.sh} script moves
XML files from the \path|ns0| directory to the two
upload directories, \path|oarc| and \path|archive|.

A second cron script, {\tt upload-data.sh} is responsible for
actually transferring XML files from the upload directories
to the remote server.    This script creates a {\em tar\/} archive
of XML files and then uploads it to the remote server with
{\tt curl}.  The script takes three commandline arguments:

\begin{verbatim}
> upload-data.sh NODE DEST URI
\end{verbatim}

{\em NODE\/} must match the name of a directory under
\path|/usr/local/dsc/run|.  Similarly, {\em DEST\/} must match the
name of a directory under \path|/usr/local/dsc/run/NODE/upload|.
{\em URI\/} is the URL/URI that the data is uploaded to.  Usually
it is just an https URL with the name of the destination server.
We also recommend running this from cron every 60 seconds.  For
example:

\begin{Verbatim}[formatcom={\footnotesize\setlength{\baselineskip}{3ex}}]
* * * * * /usr/local/dsc/libexec/upload-data.sh ns0 oarc https://collect.oarc.isc.org/
* * * * * /usr/local/dsc/libexec/upload-data.sh ns0 archive https://archive.example.com/
\end{Verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Configuring the {\dsc} Collector}

\section{dsc.conf}

Before running {\tt dsc\/} you need to create a configuration file.
Note that configuration directive lines are terminated with a semi-colon.
The configuration file currently understands the following directives:

\begin{description}

\item[local\_address]

	Specifies the DNS server's local IP address.  It is used
	to determine the ``direction'' of an IP packet: sending,
	receiving, or other.  You may specify multiple local addresses
	if necessary.

	Example: {\tt local\_address 172.16.0.1;\/}

\item[run\_dir]

	A directory that should become {\tt dsc\/}'s current directory
	after it starts.  XML files will be written here, as will
	any core dumps.

	Example: {\tt run\_dir "/var/run/dsc";\/}

\item[bpf\_program]

	A Berkeley Packet Filter program string.  Normally you
	should leave this unset.  You may use this to further
	restrict the traffic seen by {\tt dsc\/}.  Note that {\tt
	dsc\/} currently has one indexer that looks at all IP
	packets.  If you specify something like {\em udp port 53\/}
	that indexer will not work.

	Note that this directive must go before the {\em interface\/}
	directive because {\tt dsc\/} makes only one pass through
	the configuration file and the BFP filter is set when the
	interface is initialized.

	Example: {\tt bpf\_program "dst host 192.168.1.1";\/}

\item[interface]

	The interface name to sniff packets from.   You may specify multiple
	interfaces.

	Example: {\tt interface fxp0;\/}

\item[bpf\_vlan\_tag\_byte\_order]

	{\tt dsc\/} knows about VLAN tags.  Some operating systems (FreeBSD-4.x) have a bug
	whereby the VLAN tag id is byte-swapped.  Valid values for this directive
	are {\tt host\/} and {\tt net\/} (the default).    Set this to {\tt host\/}
	if you suspect your operating system has the VLAN tag byte order bug.

	Example: {\tt bpf\_vlan\_tag\_byte\_order host;\/}

\item[match\_vlan]

	A list of VLAN identifiers (integers).  If set, only the packets belonging to these
	VLANs are counted.

	Example: {\tt match\_vlan 101 102;\/}

\item[dataset]

	This directive is the hart of {\dsc}.  Please see the following section for its description.

\end{description}

\section{The dataset directive}

A {\em dataset\/} is a 2-D array of counters.  For example, you
might have a dataset with ``Query Type'' along one dimension and
``Query Name Length'' on the other.  The result is a table that
shows the distribution of query name lengths for each query type.
For example:

\vspace{1ex}
\begin{center}
\begin{tabular}{l|rrrrrr}
Len & A & AAAA & A6 & PTR & NS & SOA \\
\hline
$\cdots$ & & & & & \\
11 & 14 & 8 & 7 & 11 & 2 & 0 \\
12 & 19 & 2 & 3 & 19 & 4 & 1 \\
$\cdots$ & & & & & & \\
255 & 0 & 0 & 0 & 0 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\vspace{1ex}

\noindent
A dataset is defined by the following parameters:
\begin{itemize}
\setlength{\itemsep}{0ex plus 0.5ex minus 0.0ex}
\item A name
\item A protocol layer (IP or DNS)
\item An indexer for the first dimension
\item An indexer for the second dimension
\item Zero or more options
\end{itemize}

\noindent
{\em dsc.conf\/} syntax:

{\tt dataset\/}
{\em name\/}
{\em protocol\/}
{\em Label1:Indexer1\/}
{\em Label2:Indexer2\/}
{\em [filters]\/} 
{\em [parameters]\/};
\vspace{2ex}

\subsection{Dataset Name}

The dataset name is used in the filename for {\tt dsc\/}'s XML
files.  Although this is an opaque string, the dataset name is also
used as the name for the XML extractors.  That is, if {\dsc} finds
an XML file named {\em 1092180200.foo.xml\/}, it passes that file
to {\em foo-extractor.pl\/}.

\subsection{Protocol}

{\dsc} currently knows about two protocol layers: IP and DNS.
On the {\tt dataset\/} line they are written as {\tt ip\/} and {\tt dns\/}.

\subsection{Indexers}

An {\em indexer\/} is simply a function that transforms the attributes
of an IP/DNS message into an array index.  For some attributes the
transformation is straightforward.  For example, the ``Query Type''
indexer simply extracts the query type value from a DNS message and
uses this 16-bit value as the array index.

Other attributes are slightly more complicated.  For example, the
``TLD'' indexer extracts the TLD of the QNAME field of a DNS message
and maps it to an integer.  The indexer maintains a simple internal
table of TLD-to-integer mappings.  The actual integer values are
unimportant because the TLD strings, not the integers, appear in
the resulting XML data.

When you specify an indexer on a {\tt dataset\/} line, you must
provide both the name of the indexer and a label.  The Label appears
as an attribute in the XML output.  For example,
Figure~\ref{fig-sample-xml} shows the XML corresponding to this
{\em dataset\/} line:

\begin{verbatim}
dataset the_dataset dns Foo:foo Bar:bar queries-only;
\end{verbatim}

\begin{figure}
\begin{small}\begin{verbatim}
<array name="the_dataset" dimensions="2" start_time="1091663940" ...
  <dimension number="1" type="Foo"/>
  <dimension number="2" type="Bar"/>
  <data>
    <Foo val="1">
      <Bar val="0" count="4"/>
      ...
      <Bar val="100" count="41"/>
    </Foo>
    <Foo val="2">
      ...
    </Foo>
  </data>
</array>
\end{verbatim}\end{small}
\caption{\label{fig-sample-xml}Sample XML output}
\end{figure}

In theory you are free to choose any label that you like, however,
the XML extractors look for specific labels.  Please use the labels
given for the indexers in Tables~\ref{tbl-dns-indexers}
and~\ref{tbl-ip-indexers}.  Table~\ref{tbl-dns-indexers} shows the
currently-defined indexers for DNS messages.  Similarly,
Table~\ref{tbl-ip-indexers} shows the indexers for IP packets.

\begin{table}
\begin{center}
\begin{tabular}{|lll|}
\hline
Indexer & Label & Description \\
\hline 
certain\_qnames & CertainQnames & Popular query names seen at roots \\
cip4\_net & ClientSubnet & The client's IPv4 /24 subnet \\
client & - & The client's IPv4 address \\
do\_bit & DO & Whether the DO bit is on \\
edns\_version & EDNSVersion & The EDNS version number \\
idn\_qname & IDNQname & If the QNAME is in IDN format \\
msglen & MsgLen & The DNS message length \\
null & All & A ``no-op'' indexer \\
opcode & Opcode & DNS message opcode \\
qclass & - & Query class \\
qnamelen & QnameLen & Length of the query name \\
qtype & Qtype & DNS query type \\
query\_classification & Class & A classification for bogus queries \\
rcode & Rcode & DNS reply code \\
rd\_bit & RD & Check if Recursion Desired bit set \\
tld & TLD & TLD of the query name \\
\hline
\end{tabular}
\caption{\label{tbl-dns-indexers}DNS message indexers}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|lll|}
\hline
Indexer & Label & Description \\
\hline 
ip\_direction & Direction & one of sent, recv, or other \\
ip\_proto & IPProto & IP protocol (icmp, tcp, udp) \\
\hline
\end{tabular}
\caption{\label{tbl-ip-indexers}IP packet indexers}
\end{center}
\end{table}

\noindent
Here are some longer descriptions of the DNS indexers:

\begin{description}
\item[certain\_qnames]
	This indexer isolates the two most popular query names seen
	by DNS root servers: {\em localhost\/} and {\em
	[a--m].root-servers.net\/}.
\item[cip4\_net]
	Groups DNS messages together by the /24 subnet of the
	client's IPv4 address.  We use this to make datasets with
	large, diverse client populations more manageable and to
	provide a small amount of privacy and anonymization.
\item[client]
	The IPv4 address of the DNS client.
\item[do\_bit]
	This indexer has only two values: 0 or 1.  It indicates
	whether or not the ``DO'' bit is set in a DNS query.  According to
	RFC 2335: {\em Setting the DO bit to one in a query indicates to the server
	that the resolver is able to accept DNSSEC security RRs.}
\item[edns\_version]
	The EDNS version number, if any, in a DNS query.  EDNS
	Version 0 is documented in RFC 2671.
\item[idn\_qname]
	This indexer has only two values: 0 or 1.  It returns 1
	when the first QNAME in the DNS message question section
	is an internationalized domain name (i.e., containing
	non-ASCII characters).  Such QNAMEs begin with the string
	{\tt xn--\/}.  This convention is documented in RFC 3490.
\item[msglen]
	The overall length (size) of the DNS message.
\item[null]
	A ``no-op'' indexer that always returns the same value.
	This can be used to effectively turn the 2-D table into a
	1-D array.
\item[opcode]
	The DNS message opcode is a four-bit field.  QUERY is the
	most common opcode.  Additional currently defined opcodes
	include: IQUERY, STATUS, NOTIFY, and UPDATE.
\item[qclass]
	The DNS message query class (QCLASS) is a 16-bit value.  IN
	is the most common query class.  Additional currently defined
	query class values include: CHAOS, HS, NONE, and ANY.
\item[qnamelen]
	The length of the first (and usually only) QNAME in a DNS
	message question section.  Note this is the ``expanded''
	length if the message happens to take advantage of DNS
	message ``compression.''
\item[qtype]
	The query type (QTYPE) for the first QNAME in the DNS message
	question section.  Well-known query types include: A, AAAA,
	A6, CNAME, PTR, MX, NS, SOA, and ANY.
\item[query\_classification]
	A stateless classification of ``bogus'' queries:
	\begin{itemize}
	\setlength{\itemsep}{0ex plus 0.5ex minus 0.0ex}
	\item non-auth-tld: when the TLD is not one of the IANA-approved TLDs.
	\item root-servers.net: a query for a root server IP address.
	\item localhost: a query for the localhost IP address.
	\item a-for-root: an A query for the DNS root (.).
	\item a-for-a: an A query for an IPv4 address.
	\item rfc1918-ptr: a PTR query for an RFC 1918 address.
	\item funny-class: a query with an unknown/undefined query class.
	\item funny-qtype: a query with an unknown/undefined query type.
	\item src-port-zero: when the UDP message's source port equals zero.
	\item malformed: a malformed DNS message that could not be entirely parsed.
	\end{itemize}
\item[rcode]
	The RCODE value in a DNS reply.  The most common response
	codes are 0 (NO ERROR) and 3 (NXDOMAIN). 
\item[rd\_bit]
	This indexer returns 1 if the RD (recursion desired) bit is
	set in the query.  Usually only stub resolvers set the RD bit.
	Usually authoritative servers do not offer recursion to their
	clients.
\item[tld]
	the TLD of the first QNAME in a DNS message's question section.
\end{description}

\noindent
Here are some longer descriptions of the IP indexers:

\begin{description}
\item[ip\_direction]
	One of three values: sent, recv, or else.  Direction is determined
	based on the setting for {\em local\_address\/} in the configuration file.
\item[ip\_proto]
	The IP protocol type, e.g.: tcp, udp, icmp.
\end{description}

\subsection{Filters}

You may specify zero or more of the following filters (separated by commas) on
the {\tt dataset\/} line:

\begin{description}
\item[queries-only]
	Count only DNS query messages.  A query is a DNS message
	where the QR bit is set to 0.
\item[replies-only]
	Count only DNS reply messages.  A query is a DNS message 
        where the QR bit is set to 1.
\item[popular-qtypes]
	Count only DNS messages where the query type is one of:
	A, NS, CNAME, SOA, PTR, MX, AAAA, A6, ANY.
\item[idn-only]
	Count only DNS messages where the query name is in the
	internationalized domain name format.
\item[aaaa-or-a6-only]
	Count only DNS Messages where the query type is AAAA or A6.
\item[root-servers-net-only]
	Count only DNS messages where the query name is within
	the {\em root-servers.net\/} domain.
\end{description}

\noindent
Note that multiple filters are ANDed together.  That is, they
narrow the input stream, rather than broaden it.

\subsection{Parameters}

\noindent
{\tt dsc\/} currently supports the following optional parameters:

\begin{description}
\item[min-count={\em NN\/}]
	Cells with counts less than {\em NN\/} are not included
	in the output.  Instead, they are aggregated into the special
	values {\tt -:SKIPPED:-\/} and {\tt -:SKIPPED\_SUM:-\/}.  This helps reduce
	the size of datasets with a large number of small counts.
\end{description}

\section{A Complete Sample dsc.conf}

Here's how your entire {\em dsc.conf\/} file might look:

\noindent\hrulefill

\begin{footnotesize}
\begin{verbatim}
#bpf_program
interface em0;

local_address 192.5.5.241;

run_dir "/udir/wessels/dsc/run-pao1";

dataset qtype dns All:null Qtype:qtype queries-only;
dataset rcode dns All:null Rcode:rcode replies-only;
dataset opcode dns All:null Opcode:opcode queries-only;
dataset rcode_vs_replylen dns Rcode:rcode ReplyLen:msglen replies-only;
dataset client_subnet dns All:null ClientSubnet:cip4_net
        queries-only min-count=10;
dataset qtype_vs_qnamelen dns Qtype:qtype QnameLen:qnamelen queries-only;
dataset qtype_vs_tld dns Qtype:qtype TLD:tld queries-only,popular-qtypes
        min-count=3;
dataset certain_qnames_vs_qtype dns CertainQnames:certain_qnames Qtype:qtype
        queries-only;
dataset client_subnet2 dns Class:query_classification ClientSubnet:cip4_net
        queries-only min-count=5;

dataset idn_qname dns All:null IDNQname:idn_qname queries-only;
dataset edns_version dns All:null EDNSVersion:edns_version queries-only;
dataset do_bit dns All:null DO:do_bit queries-only;
dataset rd_bit dns All:null RD:rd_bit queries-only;

dataset idn_vs_tld dns All:null TLD:tld queries-only,idn-only;
dataset ipv6_rsn_abusers dns All:null ClientAddr:client
        queries-only,aaaa-or-a6-only,root-servers-net-only min-count=27;

dataset direction_vs_ipproto ip Direction:ip_direction IPProto:ip_proto any;
\end{verbatim}
\end{footnotesize}

\noindent\hrulefill

\section{Pushing XML files to the Presenter}

The {\tt dsc\/} process writes XML files to its current directory
(which you probably set with the {\em run\_dir\/} directive.  Back
in Section~\ref{sec-install-collector-cron} we talked about the
cron job that pushes XML files to the Presenter.

The script that we provide ({\em push-data.sh\/}) uses {\tt curl\/}
and X.509 certificates.  This is a somewhat-complex technique.  You
may want to start of with something simpler, such as {\tt rsync\/}
or {\em scp\/}, then come back and attempt HTTPS PUT later.

If you go with HTTPS PUT, the collector needs copies of the client
X.509 certificates.  The next chapter explains how to generate these
certificates.  If you use our {\em push-data.sh\/} script, make
sure that the certificate file location matches the {\em SRVAUTH\/}
setting in the script.  You need two certificate files: one contains
the Certificate Authority (CA) certificate ({\em cacert.pem\/}) and the
other contains the client (collector) certificate and private key
({\em node-cert.pem\/}).  The CA certificate is
necessary so that {\tt curl\/} can authenticate the server.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Data Storage}

\section{XML Structure}

A dataset XML file has the following structure:

\begin{verbatim}
<array name="dataset-name" dimensions="2" start_time="unix-seconds"
        stop_time="unix-seconds">
  <dimension number="1" type="Label1"/>
  <dimension number="2" type="Label2"/>
  <data>
    <Label1 val="D1-V1">
      <Label2 val="D2-V1" count="N1"/>
      <Label2 val="D2-V2" count="N2"/>
      <Label2 val="D2-V3" count="N3"/>
    </Label1>
    <Label1 val="D1-V2">
      <Label2 val="D2-V1" count="N1"/>
      <Label2 val="D2-V2" count="N2"/>
      <Label2 val="D2-V3" count="N3"/>
    </Label1>
  </data>
</array>
\end{verbatim}

\noindent
{\em dataset-name\/},
{\em Label1\/}, and
{\em Label2\/} come from the dataset definition in {\em dsc.conf\/}.

\noindent
The {\em start\_time\/} and {\em stop\_time\/} attributes
are given in Unix seconds.  They are normally 60-seconds apart.

\noindent
{\em D1-V1\/}, {\em D1-V2\/}, etc are values for the
first dimension indexer.  For some indexers these values are
numeric, for others they are strings.  If the value contains
certain non-printable characters, the string is base64-encoded
and the optional {\em base64\/} attribute is set to 1.

\noindent
Similarly, {\em D1-V1\/}, {\em D1-V2\/}, {\em D1-V2\/} are
values for the second dimension indexer.

\noindent
The {\em count\/} values are always integers.

\noindent
Here is a possibly-valid DTD for the dataset XML format.
Note, however, that the <emphasis>LABEL1</emphasis>
and <emphasis>LABEL2</emphasis> strings are different
for each dataaset:

\begin{verbatim}
<!DOCTYPE ARRAY [  

<!ELEMENT ARRAY (DIMENSION+, DATA))>
<!ELEMENT DIMENSION>
<!ELEMENT DATA (LABEL1+)>
<!ELEMENT LABEL1 (LABEL2+)>

<!ATTLIST ARRAY NAME CDATA #REQUIRED>
<!ATTLIST ARRAY DIMENSIONS CDATA #REQUIRED>
<!ATTLIST ARRAY START_TIME CDATA #REQUIRED>
<!ATTLIST ARRAY STOP_TIME CDATA #REQUIRED>
<!ATTLIST DIMENSION NUMBER CDATA #REQUIRED>
<!ATTLIST DIMENSION TYPE CDATA #REQUIRED>
<!ATTLIST LABEL1 VAL CDATA #REQUIRED>
<!ATTLIST LABEL2 VAL CDATA #REQUIRED>
<!ATTLIST LABEL2 COUNT CDATA #REQUIRED>

]> 
\end{verbatim}

\section{Archived Data Format}

{\dsc} actually uses four different file formats for archived
datasets.  These are all text-based and designed to be quickly
read from, and written to, by Perl scripts.  

\subsection{Format 1}

\noindent
\begin{tt}time $k1$ $N_{k1}$ $k2$ $N_{k2}$ $k3$ $N_{k3}$ ...
\end{tt}

\vspace{1ex}\noindent
This is a one-dimensional time-series format.\footnote{Which means
it can only be used for datasets where one of the indexers is set
to the Null indexer.}  The first column is a timestamp (unix seconds).
The remaining space-separated fields are key-value pairs.  For
example:

\begin{footnotesize}\begin{verbatim}
1093219980 root-servers.net 122 rfc1918-ptr 112 a-for-a 926 funny-qclass 16
1093220040 root-servers.net 121 rfc1918-ptr 104 a-for-a 905 funny-qclass 15
1093220100 root-servers.net 137 rfc1918-ptr 116 a-for-a 871 funny-qclass 12
\end{verbatim}\end{footnotesize}

\subsection{Format 2}

\noindent
\begin{tt}time $j1$ $k1$:$N_{j1,k1}$:$k2$:$N_{j1,k2}$:... $j2$ $k1$:$N_{j2,k1}$:$k2$:$N_{j2,k2}$:... ...
\end{tt}

\vspace{1ex}\noindent
This is a two-dimensional time-series format.  In the above,
$j$ represents the first dimension indexer and $k$ represents
the second.  Key-value pairs for the second dimension are
separated by colons, rather than space.  For example:

\begin{footnotesize}\begin{verbatim}
1093220160 recv icmp:2397:udp:136712:tcp:428 sent icmp:819:udp:119191:tcp:323
1093220220 recv icmp:2229:udp:124708:tcp:495 sent icmp:716:udp:107652:tcp:350
1093220280 recv udp:138212:icmp:2342:tcp:499 sent udp:120788:icmp:819:tcp:364
1093220340 recv icmp:2285:udp:137107:tcp:468 sent icmp:733:udp:118522:tcp:341
\end{verbatim}\end{footnotesize}

\subsection{Format 3}

\noindent
\begin{tt}$k$ $N_{k}$
\end{tt}

\vspace{1ex}\noindent
This format is used for one-dimensional datasets where the key space
is (potentially) very large.  That is, putting all the key-value pairs
on a single line would result in a very long line in the datafile.
Furthermore, for these larger datasets, it is prohibitive to
store the data as a time series.  Instead the counters are incremented
over time.  For example:

\begin{verbatim}
10.0.160.0 3024
10.0.20.0 92
10.0.244.0 5934
\end{verbatim}

\subsection{Format 4}

\noindent
\begin{tt}$j$ $k$ $N_{j,k}$
\end{tt}

\vspace{1ex}\noindent
This format is used for two-dimensional datasets where one or both
key spaces are very large.  Again, counters are incremented over
time, rather than storing the data as a time series.
For example:

\begin{verbatim}
10.0.0.0 non-auth-tld 105
10.0.0.0 ok 37383
10.0.0.0 rfc1918-ptr 5941
10.0.0.0 root-servers.net 1872
10.0.1.0 a-for-a 6
10.0.1.0 non-auth-tld 363
10.0.1.0 ok 144
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Bugs}

\begin{itemize}

\item
	Seems too confusing to have an opaque name for indexers in
	dsc.conf dataset line.  The names are pre-determined anyway
	since they must match what the XML extractors look for.

\item
	DSC perl modules are installed in the ``site\_perl'' directory
	but they should probably be installed under /usr/local/dsc.

\end{itemize}

\end{document}
